介绍背景、承担角色、技术栈、项目成果、项目总结、遇到的问题

### 1.微信扫码登陆功能

- 准备一个企业的公众号，在后台创建一个新的应用（名称），配置一个返回地址（域名），获取对应的appid和app_secret

- 生成微信扫描二维码

  - 在yml文件中，配置appid，app_secret，redirect_url参数

  - 编一一个配置类，用于获取这三个参数

    ```java
    @Component
    public class ConstantPropertiesUtil implements InitializingBean {
    
        @Value("${wx.open.app_id}")
        private String appId;
    }
    ```
  
  - 编写一个Controller类，写一个方法，获得微信扫面二维码界面，参数有appid，scope（固定值），redirect_url,state(当前时间)，通过，https://open.weixin.qq.com/connect/qrconnect?appid=APPID&redirect_uri=REDIRECT_URI&response_type=code&scope=SCOPE&state=STATE这个方法，将我们的参数通过这个地址返回到腾讯的服务器，此时，就会显示我们的微信二维码扫描界面了。
  
- 用我们的微信扫描这个二维码，点击确认之后，通过微信后台配置的回调地址，我们可以拿到一个code参数和一个state参数

  - 通过回调域名地址，跳转到本地方法接口里面

- 编写回调方法，获取扫描人信息

- 编写一个方法解析code参数，通过一个域名（https://api.weixin.qq.com/sns/oauth2/access_token），将我们的appid，secret，code和grant_type=authorization_code作为参数出入，获得access_token和openid

  - 通过openid判断我们的用户数据是否存在了我们的数据库中。

- 通过一个地址（https://api.weixin.qq.com/sns/sns/userinfo），将我们的access_token和openid作为参数传入一个地址，就能够获得我们对应的对用的用户信息了（openid，name，sex,头像，省份）。

- 将用户信息存入到数据库中。

### 2.项目中遇到的问题

> 标签获取的问题

在写到文章标签的修改功能时，点击其他标签修改按钮都只能获取到第一个的值；

原因：这是id循环的问题，因为在js函数如果有多个相同的id默认获取第一个id的值，我原来是这样写的，所以导致点击其他标签总是只能获取到第一个的值。

解决：解决的办法之一就是使每个id成为唯一值；在id值上加上循环出来的文章类型id值使其成为唯一值

> 跳转界面的问题

能成功验证用户名和密码，但是验证成功跳转页面时出现错误，总是提示不支持post请求；一开始是以为是自定义的视图解析器错误，设置的跳转路径没有问题啊

解决：把自定义成功登陆处理类里的请求转发改成重定向就解决了；

> 转发和重定向的区别

- 重定向时浏览器上的网址改变；转发是浏览器上的网址不变
- 重定向实际上产生了多次请求；转发只有一次请求
- 重定向时的网址可以是任何网址；转发的网址必须是本站点的网址

### 3.文本分类

>  word2vec简介

在 NLP 中，把 x 看做一个句子里的一个词语，y 是这个词语的上下文词语，那么这里的 f，便是 NLP 中经常出现的『语言模型』（language model），这个模型的目的，就是判断 (x,y) 这个样本，是否符合自然语言的法则，更通俗点说就是：词语x和词语y放在一起，是不是人话。

Word2vec 正是来源于这个思想，但它的最终目的，不是要把 f 训练得多么完美，而是只关心模型训练完后的副产物——模型参数（这里特指神经网络的权重），并将这些参数，作为输入 x 的某种向量化的表示，这个向量便叫做——词向量（这里看不懂没关系，下一节我们详细剖析）。

我们来看个例子，如何用 Word2vec 寻找相似词：

- 对于一句话：『她们 夸 吴彦祖 帅 到 没朋友』，如果输入 x 是『吴彦祖』，那么 y 可以是『她们』、『夸』、『帅』、『没朋友』这些词
- 现有另一句话：『她们 夸 我 帅 到 没朋友』，如果输入 x 是『我』，那么不难发现，这里的上下文 y 跟上面一句话一样
- 从而 f(吴彦祖) = f(我) = y，所以大数据告诉我们：我 = 吴彦祖（完美的结论）

上面我们提到了语言模型

- 如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』
- 而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』

> tfidf缺点

IDF的固有缺点：简单结构并不能使提取的关键词，十分有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能。尤其是在同类语料库中，这一方法有很大弊端，往往一些同类文本的关键词被掩盖。例如：语料库 中教育类文章偏多，而文本 是一篇属于教育类的文章，那么教育类相关的词语的值将会偏小，使提取文本关键词的召回率更低。

因此，需要考虑以下 2 个问题:(1)在所有文本中词频高的词语如果也是每个类别中词频高的词语，那么这些词语对于辨别分类的贡献度很低甚至不能作为判断依据，这和分词时使用的停用词表有相似的缘由。(2)如果某些词语能鲜明地使得某个样本标记为某类别，但是这些词语仅仅存在于很少的样本中，那么这些词语应该当作特例来看待，不加入到词表中。基于以上 2 个问题，本文通过适当调整 TF － IDF算法，提出如下计算方法:
